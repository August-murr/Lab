{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LyslRvf8HA_"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install bitsandbytes peft trl --quiet\n",
        "!pip install --upgrade datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjgCMmVY8S-T"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uANiNEm98S8R"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = load_dataset(\"August4293/Preference-Dataset\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTW-yJRu8S3f"
      },
      "outputs": [],
      "source": [
        "# Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    example[\"processed\"] = \"<s>[INST] \" + example['prompt'] + \" [/INST] \" + example['chosen'] + \"</s>\"\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6wPRlln8S1D"
      },
      "outputs": [],
      "source": [
        "data = data.map(preprocess_data).train_test_split(test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfUjxwdO8SzF"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and eval\n",
        "train_dataset = data['train'].rename_column('processed','text')\n",
        "eval_dataset = data['test'].rename_column('processed','text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-DNsKp8Svv"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns\n",
        "train_dataset = train_dataset.remove_columns(['prompt', 'rejected', 'chosen'])\n",
        "eval_dataset = eval_dataset.remove_columns(['prompt', 'rejected', 'chosen'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-4QBUPa8SuF"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "user_secrets = UserSecretsClient()\n",
        "wandb_token = user_secrets.get_secret(\"wandb_august\")\n",
        "HF_token = user_secrets.get_secret(\"HF_august\")\n",
        "login(HF_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2WSRtPW8Sst"
      },
      "outputs": [],
      "source": [
        "# Initialize WandB\n",
        "wandb.login(key = wandb_token)\n",
        "run = wandb.init(\n",
        "    project='mistral self-alignment',\n",
        "    job_type=\"training\",\n",
        "    name=\"test run\",\n",
        "    notes=f\"Initial SFT run on full dataset of {len(train_dataset)} and 1 epoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT3ESVB78osv"
      },
      "outputs": [],
      "source": [
        "# Initialize base model\n",
        "base_model = (\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtzuTOZ8oqN"
      },
      "outputs": [],
      "source": [
        "# Configure BitsAndBytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igclr1Ew8onx"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U3l7ta48old"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ECFq22n8oi9"
      },
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8nWcAmM8ogg"
      },
      "outputs": [],
      "source": [
        "# Configure PEFT\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxdUHWsk8od8"
      },
      "outputs": [],
      "source": [
        "# Get PEFT model\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_0UQ5y48obc"
      },
      "outputs": [],
      "source": [
        "# Calculate total trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in PEFT adapter: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdoXvGij8oYq"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/checkpoints\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=300,\n",
        "    logging_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P43Q67Ug8oWM"
      },
      "outputs": [],
      "source": [
        "# Initialize SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=2048,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    dataset_text_field=\"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo3KhCs287DR"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJSMhy-u87Ay"
      },
      "outputs": [],
      "source": [
        "# Save fine-tuned model\n",
        "fine_tuned_model_name = \"mistral_self_alignment_SFT\"\n",
        "trainer.model.save_pretrained(fine_tuned_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDOve32V86-k"
      },
      "outputs": [],
      "source": [
        "# Define commit message\n",
        "commit_message = \"Initial adapter with SFT on full dataset and 1 epoch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpgNLhx2868Z"
      },
      "outputs": [],
      "source": [
        "# Push model to Hugging Face Hub\n",
        "trainer.model.push_to_hub(fine_tuned_model_name, commit_message=commit_message, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(fine_tuned_model_name, commit_message=commit_message, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An2thEL8865h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
