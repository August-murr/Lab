{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pymongo[srv] bitsandbytes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pymongo import MongoClient, UpdateOne\nfrom pymongo.server_api import ServerApi\nfrom kaggle_secrets import UserSecretsClient\nfrom datetime import datetime\nimport time\nimport torch\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\nimport threading\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:09.431833Z","iopub.execute_input":"2024-05-29T07:06:09.432160Z","iopub.status.idle":"2024-05-29T07:06:15.888413Z","shell.execute_reply.started":"2024-05-29T07:06:09.432131Z","shell.execute_reply":"2024-05-29T07:06:15.887432Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Constants\nBATCH_SIZE = 48\ninstance_name = \"valerie_1_2xt4\"\nRUN_COLLECTION_NAME = 'base_mistral_train_run'  # Replace with your run collection name","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:15.889813Z","iopub.execute_input":"2024-05-29T07:06:15.890791Z","iopub.status.idle":"2024-05-29T07:06:15.895102Z","shell.execute_reply.started":"2024-05-29T07:06:15.890753Z","shell.execute_reply":"2024-05-29T07:06:15.894233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Connect to MongoDB\nuser_secrets = UserSecretsClient()\nmongo_url = user_secrets.get_secret(\"mongodb\")\nclient = MongoClient(mongo_url, server_api=ServerApi('1'))\ndb = client['gsm8k_dataset']  # Replace with your database name\ncollection = db[RUN_COLLECTION_NAME]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:15.896220Z","iopub.execute_input":"2024-05-29T07:06:15.896552Z","iopub.status.idle":"2024-05-29T07:06:16.192363Z","shell.execute_reply.started":"2024-05-29T07:06:15.896513Z","shell.execute_reply":"2024-05-29T07:06:16.191575Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Fetch and update a batch of documents atomically\ndef fetch_batch(instance_name):\n    current_time = datetime.utcnow()\n\n    # Create an aggregation pipeline to fetch and update documents atomically\n    pipeline = [\n        {\"$match\": {\"status\": \"pending\"}},\n        {\"$limit\": BATCH_SIZE},\n        {\"$set\": {\"status\": \"in progress\", \"instance\": instance_name, \"start_time\": current_time}},\n        {\"$merge\": {\"into\": RUN_COLLECTION_NAME, \"whenMatched\": \"merge\", \"whenNotMatched\": \"fail\"}}\n    ]\n\n    # Run the aggregation pipeline\n    collection.aggregate(pipeline)\n\n    # Fetch the documents that were just updated\n    batch = collection.find({\n        \"status\": \"in progress\",\n        \"instance\": instance_name\n    })\n\n    return list(batch)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:16.194657Z","iopub.execute_input":"2024-05-29T07:06:16.194965Z","iopub.status.idle":"2024-05-29T07:06:16.201248Z","shell.execute_reply.started":"2024-05-29T07:06:16.194941Z","shell.execute_reply":"2024-05-29T07:06:16.200376Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to check if there are any pending batches\ndef has_pending_batches():\n    pending_count = collection.count_documents({\"status\": \"pending\"})\n    return pending_count > 0","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:16.202527Z","iopub.execute_input":"2024-05-29T07:06:16.203045Z","iopub.status.idle":"2024-05-29T07:06:16.212820Z","shell.execute_reply.started":"2024-05-29T07:06:16.203014Z","shell.execute_reply":"2024-05-29T07:06:16.212110Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:16.213919Z","iopub.execute_input":"2024-05-29T07:06:16.214689Z","iopub.status.idle":"2024-05-29T07:06:16.222856Z","shell.execute_reply.started":"2024-05-29T07:06:16.214659Z","shell.execute_reply":"2024-05-29T07:06:16.222034Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:16.223858Z","iopub.execute_input":"2024-05-29T07:06:16.224239Z","iopub.status.idle":"2024-05-29T07:06:16.233244Z","shell.execute_reply.started":"2024-05-29T07:06:16.224192Z","shell.execute_reply":"2024-05-29T07:06:16.232421Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:06:16.234317Z","iopub.execute_input":"2024-05-29T07:06:16.234652Z","iopub.status.idle":"2024-05-29T07:06:16.397847Z","shell.execute_reply.started":"2024-05-29T07:06:16.234616Z","shell.execute_reply":"2024-05-29T07:06:16.396862Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load models on GPU 0 and GPU 1\nmodel_0 = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    torch_dtype=torch.float16,\n    device_map=\"cuda:0\",\n    quantization_config=quantization_config)\nmodel_1 = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    torch_dtype=torch.float16,\n    device_map=\"cuda:1\",\n    quantization_config=quantization_config\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to tokenize the inputs and move to respective device\ndef tokenize_inputs(data, device):\n    questions = [item['tagged_question'] for item in data]\n    return tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:08:20.530699Z","iopub.execute_input":"2024-05-29T07:08:20.531123Z","iopub.status.idle":"2024-05-29T07:08:20.536809Z","shell.execute_reply.started":"2024-05-29T07:08:20.531096Z","shell.execute_reply":"2024-05-29T07:08:20.535921Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define functions to run the model generation in batches\ndef generate_responses(model, inputs, device, batch_size):\n    responses = []\n    num_batches = math.ceil(len(inputs.input_ids) / batch_size)\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(inputs.input_ids))\n        batch_input_ids = inputs.input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = inputs.attention_mask[start_idx:end_idx].to(device)\n        batch_responses = model.generate(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask,\n            max_length=800,\n            do_sample=True,\n            temperature=0.05,\n            top_p=0.95,\n            top_k=60,\n            repetition_penalty=1.15,\n            num_return_sequences=1\n        )\n        decoded_responses = [tokenizer.decode(response, skip_special_tokens=True) for response in batch_responses]\n        responses.extend(decoded_responses)\n        torch.cuda.empty_cache()\n    return responses","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:08:20.537895Z","iopub.execute_input":"2024-05-29T07:08:20.538250Z","iopub.status.idle":"2024-05-29T07:08:20.547592Z","shell.execute_reply.started":"2024-05-29T07:08:20.538226Z","shell.execute_reply":"2024-05-29T07:08:20.546668Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define the threading functions for each model\ndef generate_responses_0():\n    global responses_0\n    responses_0 = generate_responses(model_0, inputs_0, \"cuda:0\", batch_size=BATCH_SIZE/2)\n\ndef generate_responses_1():\n    global responses_1\n    responses_1 = generate_responses(model_1, inputs_1, \"cuda:1\", batch_size=BATCH_SIZE/2)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:08:20.548718Z","iopub.execute_input":"2024-05-29T07:08:20.549036Z","iopub.status.idle":"2024-05-29T07:08:20.558537Z","shell.execute_reply.started":"2024-05-29T07:08:20.549011Z","shell.execute_reply":"2024-05-29T07:08:20.557810Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def update_responses(responses, instance_name):\n    bulk_ops = []\n    for doc_id, response in responses:\n        bulk_ops.append(UpdateOne(\n            {\"_id\": doc_id},\n            {\n                \"$set\": {\n                    \"response\": response,\n                    \"status\": \"completed\",\n                    \"instance\": instance_name\n                }\n            }\n        ))\n\n    collection.bulk_write(bulk_ops)\n    print(f\"Updated responses for instance {instance_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T07:08:20.562089Z","iopub.execute_input":"2024-05-29T07:08:20.562417Z","iopub.status.idle":"2024-05-29T07:08:20.572307Z","shell.execute_reply.started":"2024-05-29T07:08:20.562394Z","shell.execute_reply":"2024-05-29T07:08:20.571461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"while has_pending_batches():\n    batch = fetch_batch(instance_name=instance_name)\n    # Prepare data for processing\n    data = [{\"_id\": doc[\"_id\"], \"tagged_question\": f\"<s>[INST]{doc['question']} \\n do it step by step[/INST] \"} for doc in batch]\n    len_data = len(data)\n    data_0 = data[:len_data//2]\n    data_1 = data[len_data//2:]\n    \n    inputs_0 = tokenize_inputs(data_0, \"cuda:0\")\n    inputs_1 = tokenize_inputs(data_1, \"cuda:1\")\n\n    # Create threads for each function\n    thread_0 = threading.Thread(target=generate_responses_0)\n    thread_1 = threading.Thread(target=generate_responses_1)\n\n    # Start the threads\n    thread_0.start()\n    thread_1.start()\n\n    # Wait for both threads to complete\n    thread_0.join()\n    thread_1.join()\n    \n    # Now responses_0, responses_1, batch_times_0, and batch_times_1 should be available\n    responses_0 = [(data_0[i][\"_id\"], responses_0[i]) for i in range(len(responses_0))]\n    responses_1 = [(data_1[i][\"_id\"], responses_1[i]) for i in range(len(responses_1))]\n    all_responses = responses_0 + responses_1\n    \n    update_responses(all_responses, instance_name)\n\n    print(f\"Instance {instance_name} completed a batch.\")","metadata":{},"execution_count":null,"outputs":[]}]}